{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HackerEarth Deep Learning challenge: Identify the dance form",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1UH4VLoP7zJdAVS16AXvUHyyck63gHPPj",
      "authorship_tag": "ABX9TyMpuzDap1SEEmbFXUiW2eVs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atik-Ahamed/HackerEarth-Deep-Learning-cha...nge-Identify-the-dance-form/blob/master/HackerEarth_Deep_Learning_challenge_Identify_the_dance_form.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o978mRm-GdD0",
        "colab_type": "text"
      },
      "source": [
        "# Importing files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfhRgQW9UT4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import cv2\n",
        "from sklearn import metrics\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets, models\n",
        "import numpy as np\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "import math\n",
        "from PIL import Image\n",
        "from IPython.display import clear_output\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from tqdm import tnrange\n",
        "import matplotlib as mpl\n",
        "import matplotlib.cm as cm\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.nn  import functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "from math import pi\n",
        "from math import cos\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-w1oeMWSGqZW",
        "colab_type": "text"
      },
      "source": [
        "# CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCjVvDluGs0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name ='resnet50' #current model \n",
        "#model_name = 'unet'\n",
        "pretrained =True \n",
        "W=640 #image width\n",
        "H=480 #image height\n",
        "split=.15\n",
        "random_state=1000\n",
        "PATH='/content/drive/My Drive/ml/HackerEarthDanceMove/models/' #this is where the models will be saved after training\n",
        "SUB_PATH='/content/drive/My Drive/ml/HackerEarthDanceMove/subfiles/' #Here the submission files will be saved\n",
        "EPOCH= 100\n",
        "LR=1e-3 #Initial Learning Rate\n",
        "batch_size = 1 \n",
        "STEP_SIZE=30\n",
        "c_w=False #Class weight to balance imbalenced data\n",
        "cur_time_stamp=str(time.time()) #every time unique time is generated, hene the model and submission will be renamed according to this\n",
        "saved_model_name=PATH+cur_time_stamp+'_'+model_name\n",
        "saved_submission_file=SUB_PATH+cur_time_stamp+'_'+'sub.csv'\n",
        "#snapshot ensembling settings\n",
        "epochs_per_cycle=100 \n",
        "cycles=5\n",
        "SE=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mJRsJ7EGh2n",
        "colab_type": "text"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaKcEAHKlS9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.listdir('/content/drive/My Drive/datasets')\n",
        "\"\"\"Unzipping the downloaded dataset\"\"\"\n",
        "!unzip -q '/content/drive/My Drive/ml/HackerEarthDanceMove/HackerEarthDanceMoveChallenge.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13LfwaAPuVSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df=pd.read_csv('/content/dataset/train.csv')\n",
        "test_df=pd.read_csv('/content/dataset/test.csv')\n",
        "print(train_df.head())\n",
        "print(train_df.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li9rc9RcUW3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['target'].value_counts().plot(kind='bar',color='grey')\n",
        "plt.title('target class distribution')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX8GtTiEnWkQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data=train_df['Image']\n",
        "y_data=train_df['target']\n",
        "y_data=np.array(y_data)\n",
        "class_weights=None\n",
        "if c_w==True:\n",
        "  class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(y_data),\n",
        "                                                 y_data)\n",
        "  print('The class weights are')\n",
        "  print(class_weights)  \n",
        "\n",
        "le = LabelEncoder()\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "y_data = le.fit_transform(y_data)\n",
        "y_data = ohe.fit_transform(y_data.reshape(-1,1))\n",
        "#print(y_data.shape)\n",
        "#print(y_data)\n",
        "\n",
        "inv_y_data = ohe.inverse_transform(y_data)\n",
        "inv_y_data = le.inverse_transform(inv_y_data.astype(int).ravel())\n",
        "#print(inv_y_data)\n",
        "\n",
        "\n",
        "\n",
        "#y_data=pd.get_dummies(y_data)\n",
        "x_train,x_val,y_train,y_val=train_test_split(x_data,y_data,test_size=split,random_state=random_state)\n",
        "\n",
        "y_train,y_val=np.array(y_train),np.array(y_val)\n",
        "\n",
        "print(x_train.shape,y_train.shape,x_val.shape,y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ax7zf7BVO4",
        "colab_type": "text"
      },
      "source": [
        "# Datagenerator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBpDVkV9VE2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generate_data(Dataset):\n",
        "    def __init__(self,length,data_type,transform=None):\n",
        "\n",
        "        self.transform = transform\n",
        "        self.length=length\n",
        "        self.data_type=data_type\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.data_type=='train':\n",
        "          image=Image.open('/content/dataset/train/'+x_train.iloc[idx])    \n",
        "          if self.transform:\n",
        "            image=self.transform(image)\n",
        "          #print(image.shape)\n",
        "          label=y_train[idx]\n",
        "          return [image,label]\n",
        "        \n",
        "        if self.data_type=='val':\n",
        "          image=Image.open('/content/dataset/train/'+x_val.iloc[idx])    \n",
        "          if self.transform:\n",
        "            image=self.transform(image)\n",
        "          label=y_val[idx]\n",
        "          return [image,label]\n",
        "        \n",
        "        if self.data_type=='test':\n",
        "          image=Image.open('/content/dataset/test/'+test_df.iloc[idx]['Image'])    \n",
        "          if self.transform:\n",
        "            image=self.transform(image)      \n",
        "          return image\n",
        "\n",
        "\n",
        "trans_train = transforms.Compose([\n",
        "                                  \n",
        "    transforms.Resize((W,H)),\n",
        "\n",
        "    \n",
        "    #Play with some augmentations check what what works better\n",
        "    \n",
        "    #torchvision.transforms.RandomChoice([\n",
        "    #torchvision.transforms.ColorJitter(brightness=10, contrast=10),\n",
        "    #transforms.RandomRotation(20),\n",
        "    #torchvision.transforms.RandomHorizontalFlip(),\n",
        "    #torchvision.transforms.RandomCrop(2),\n",
        "    #torchvision.transforms.RandomVerticalFlip(),\n",
        "    #torchvision.transforms.RandomAffine((-5,5)),\n",
        "    #]), \n",
        "   # ImageNetPolicy(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #imagenet normalization policy check if it works better or not\n",
        "])\n",
        "\n",
        "trans_val = transforms.Compose([\n",
        "    transforms.Resize((W,H)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_set = Generate_data(length=x_train.shape[0],data_type='train', transform = trans_train)\n",
        "val_set = Generate_data(length=x_val.shape[0],data_type='val', transform = trans_val)\n",
        "test_set = Generate_data(length=test_df.shape[0],data_type='test',transform=trans_val)\n",
        "image_datasets = {\n",
        "    'train': train_set, 'val': val_set, 'test':test_set\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "dataloader = {\n",
        "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
        "    'test': DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3egAYe05q7o2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs,label=next(iter(dataloader['train']))\n",
        "def reverse_transform(inp):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    #print(inp.shape)\n",
        "    if pretrained == True:\n",
        "      mean = np.array([0.485, 0.456, 0.406])\n",
        "      std = np.array([0.229, 0.224, 0.225])\n",
        "      inp = std * inp + mean\n",
        "    #inp = np.clip(inp, 0, 1)\n",
        "    \n",
        "    inp = (inp * 255).astype(np.uint8)\n",
        "    #inp=np.reshape(inp,(inp.shape[0],inp.shape[1]))\n",
        "\n",
        "    return inp\n",
        "\n",
        "#print(inputs.shape)\n",
        "plt.imshow(reverse_transform(inputs[0]))\n",
        "\n",
        "label_data = ohe.inverse_transform(label)\n",
        "label_data = le.inverse_transform(label_data.astype(int).ravel())\n",
        "plt.title(label_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py0Gu633PKuF",
        "colab_type": "text"
      },
      "source": [
        "# UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qp9ccubPMkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=3, out_channels=8, init_features=32):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        features = init_features\n",
        "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(\n",
        "            features * 16, features * 8, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n",
        "        self.upconv3 = nn.ConvTranspose2d(\n",
        "            features * 8, features * 4, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n",
        "        self.upconv2 = nn.ConvTranspose2d(\n",
        "            features * 4, features * 2, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n",
        "        self.upconv1 = nn.ConvTranspose2d(\n",
        "            features * 2, features, kernel_size=2, stride=2\n",
        "        )\n",
        "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
        "\n",
        "        self.conv = nn.Conv2d(\n",
        "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
        "        )\n",
        "        self.sig=torch.nn.Sigmoid()\n",
        "        self.fc=nn.Linear(2457600,8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.encoder1(x)\n",
        "        enc2 = self.encoder2(self.pool1(enc1))\n",
        "        enc3 = self.encoder3(self.pool2(enc2))\n",
        "        enc4 = self.encoder4(self.pool3(enc3))\n",
        "\n",
        "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
        "\n",
        "        dec4 = self.upconv4(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.decoder4(dec4)\n",
        "        dec3 = self.upconv3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.decoder3(dec3)\n",
        "        dec2 = self.upconv2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.decoder2(dec2)\n",
        "        dec1 = self.upconv1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.decoder1(dec1)\n",
        "        conv1= self.conv(dec1)\n",
        "        conv1=conv1.view(-1,conv1.shape[1]*conv1.shape[2]*conv1.shape[3])\n",
        "        out=self.fc(conv1)\n",
        "        #print(conv1.shape)\n",
        "        #out=self.sig(conv1)\n",
        "\n",
        "        \n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def _block(in_channels, features, name):\n",
        "        return nn.Sequential(\n",
        "            OrderedDict(\n",
        "                [\n",
        "                    (\n",
        "                        name + \"conv1\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=in_channels,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
        "                    (\n",
        "                        name + \"conv2\",\n",
        "                        nn.Conv2d(\n",
        "                            in_channels=features,\n",
        "                            out_channels=features,\n",
        "                            kernel_size=3,\n",
        "                            padding=1,\n",
        "                            bias=False,\n",
        "                        ),\n",
        "                    ),\n",
        "                    (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n",
        "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
        "                ]\n",
        "            )\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scuztsAt4gbQ",
        "colab_type": "text"
      },
      "source": [
        "# Getting The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAn_sBhDvLLH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(model_name):\n",
        "    \n",
        "  if model_name=='resnet50':\n",
        "    if pretrained==True:\n",
        "      model_ft = models.resnet50(pretrained=True)\n",
        "    else:\n",
        "      model_ft = models.resnet50(pretrained=False)\n",
        "  if model_name=='unet':\n",
        "    model=UNet()\n",
        "    if use_cuda:\n",
        "      model=model.cuda()\n",
        "    return model\n",
        "\n",
        "  #model_ft = models.resnet50(pretrained=False)\n",
        "  #model_ft=models.resnet152(pretrained=True)\n",
        "  #model_ft=models.resnet18(pretrained=True)\n",
        "  #print(summary(model_ft, input_size=(3, 256, 256)))\n",
        "  num_ftrs = model_ft.fc.in_features\n",
        "  #print(num_ftrs)\n",
        "\n",
        "  model_ft.fc = nn.Linear(num_ftrs, y_train.shape[1])\n",
        "  model=model_ft\n",
        "  if(use_cuda):\n",
        "    model=model.cuda()\n",
        "  del model_ft\n",
        "  #print(summary(model, input_size=(3, W, H)))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yet2Hh0kd8-d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "model=get_model(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfSXvOcf2ANc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = pretrainedmodels.__dict__[model_name](num_classes=8,pretrained=)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqLxI5tszd5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(summary(model,(3,640,480)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ9-Vn_C4Axm",
        "colab_type": "text"
      },
      "source": [
        "# Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkj_OJrrv2D_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_loss(preds,labels,metrics,class_weights=None):\n",
        "    #class_weights=torch.from_numpy(class_weights)\n",
        "    if c_w==True:\n",
        "      if use_cuda:\n",
        "        class_weights=torch.cuda.FloatTensor(class_weights)\n",
        "      ce_loss=torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "      ce_loss=torch.nn.CrossEntropyLoss()\n",
        "    labels=torch.from_numpy(np.argmax(labels.data.cpu().numpy(),axis=1)).cuda()\n",
        "    \n",
        "    loss=ce_loss(preds,labels)\n",
        "    metrics['loss'] += loss.data.cpu().numpy() * labels.size(0)\n",
        "    return loss\n",
        "\n",
        "def calc_acc(preds,labels,metrics):\n",
        "    _, preds = torch.max(preds, 1)#pred contains the max valued index\n",
        "    labels=torch.from_numpy(np.argmax(labels.data.cpu().numpy(),axis=1)).cuda()\n",
        "    #print(preds.shape,labels.shape)\n",
        "    metrics['Acc.'] += torch.sum(preds == labels.data)\n",
        "\n",
        "def calc_f1_score(preds,labels,metrics):\n",
        "    _, preds = torch.max(preds, 1)#pred contains the max valued index\n",
        "    preds=preds.data.cpu().numpy()\n",
        "    labels=np.argmax(labels.data.cpu().numpy(),axis=1)\n",
        "    #print(preds.shape,labels.shape)\n",
        "    f1=f1_score(labels,preds,average = 'macro')\n",
        "    f1=f1* labels.shape[0]\n",
        "    metrics['F1_Score'] +=f1\n",
        "\n",
        "    \n",
        "def print_metrics(metrics, epoch_samples, phase):\n",
        "    outputs = []\n",
        "    for k in metrics.keys():\n",
        "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
        "\n",
        "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKvnGmoA4Lzw",
        "colab_type": "text"
      },
      "source": [
        "# Train Functionality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0xSAn0n4Oif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 1e10\n",
        "    best_acc=-1\n",
        "    best_f1 = -1\n",
        "\n",
        "    for epoch in tnrange(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('~' * 10)\n",
        "\n",
        "        since = time.time()\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                for param_group in optimizer.param_groups:\n",
        "                    print(\"LR\", param_group['lr'])\n",
        "\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            metrics = defaultdict(float)\n",
        "            epoch_samples = 0\n",
        "\n",
        "            for inputs, labels in dataloader[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = calc_loss(outputs, labels, metrics,class_weights)\n",
        "                    calc_acc(outputs, labels,metrics)#no return just calc Acc.\n",
        "                    calc_f1_score(outputs,labels,metrics)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                epoch_samples += inputs.size(0) #number of total samples in one epoch\n",
        "\n",
        "            #after each epoch ends the following lines prints and measures the metrics\n",
        "            print_metrics(metrics, epoch_samples, phase)\n",
        "            epoch_loss = metrics['loss'] / epoch_samples\n",
        "            epoch_acc = metrics['Acc.'] / epoch_samples\n",
        "            epoch_f1 = metrics['F1_Score'] / epoch_samples\n",
        "            \n",
        "            # deep copy the model\n",
        "            if phase == 'val':\n",
        "              if epoch_loss<best_loss:\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                best_loss=epoch_loss\n",
        "              if epoch_acc>best_acc:\n",
        "                best_acc=epoch_acc\n",
        "              if epoch_f1>best_f1:\n",
        "                best_f1=epoch_f1               \n",
        "                \n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        #print('val loss: ',epoch_loss,' val acc: ',epoch_acc,' val f1 ',epoch_f1)\n",
        "\n",
        "\n",
        "    print('Best val loss: {:4f}'.format(best_loss))\n",
        "    print('Best val acc: {:4f}'.format(best_acc))\n",
        "    print('Best val f1: {:4f}'.format(best_f1))\n",
        "   \n",
        "     # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHG6wFV4U_R",
        "colab_type": "text"
      },
      "source": [
        "# Training and validation Specific model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYlfwFJP4UxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Model is trained with adam optimizer and saved everytime if current epoch loss is less than the untill observed best validation loss\n",
        "\"\"\"\n",
        "if SE==False:\n",
        "  optimizer_ft = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
        "\n",
        "  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=STEP_SIZE, gamma=0.1)\n",
        "\n",
        "  model = train_model(optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWyjZONh_QcV",
        "colab_type": "text"
      },
      "source": [
        "# Snapshot ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-8dsQRaFjkc",
        "colab_type": "text"
      },
      "source": [
        "## Functionality for training SE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR7McHzr_Tvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def proposed_lr(initial_lr, iteration, epoch_per_cycle):\n",
        "    # proposed learning late function\n",
        "    return initial_lr * (cos(pi * iteration / epoch_per_cycle) + 1) / 2\n",
        "\n",
        "def train_se(epochs_per_cycle, cycles, initial_lr, train_loader, vis=None):\n",
        "\n",
        "    snapshots = []\n",
        "    _lr_list, _loss_list = [], []\n",
        "    count = 0\n",
        "    #epochs_per_cycle = epochs // cycles\n",
        "    optimizer = optim.SGD(model.parameters(), lr=initial_lr)\n",
        "\n",
        "    for i in range(cycles):\n",
        "\n",
        "\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        best_loss = 1e10\n",
        "        best_acc=-1\n",
        "        best_f1 = -1\n",
        "        for j in range(epochs_per_cycle):\n",
        "            lr = proposed_lr(initial_lr, j, epochs_per_cycle)\n",
        "            for param_group in optimizer.param_groups:\n",
        "              param_group['lr']=lr\n",
        "            #optimizer.state_dict()[\"param_groups\"][0][\"lr\"] = lr\n",
        "\n",
        "\n",
        "\n",
        "            print('Cycle {} Epoch {}/{}'.format(i+1,j+1, epochs_per_cycle))\n",
        "            print('~' * 10)\n",
        "\n",
        "            since = time.time()\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        print(\"LR\", param_group['lr'])\n",
        "\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                metrics = defaultdict(float)\n",
        "                epoch_samples = 0\n",
        "\n",
        "                for inputs, labels in dataloader[phase]:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    \n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        loss = calc_loss(outputs, labels, metrics,class_weights)\n",
        "                        calc_acc(outputs, labels,metrics)#no return just calc Acc.\n",
        "                        calc_f1_score(outputs,labels,metrics)\n",
        "\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    epoch_samples += inputs.size(0) #number of total samples in one epoch\n",
        "\n",
        "                    #after each epoch ends the following lines prints and measures the metrics\n",
        "                print_metrics(metrics, epoch_samples, phase)\n",
        "                epoch_loss = metrics['loss'] / epoch_samples\n",
        "                epoch_acc = metrics['Acc.'] / epoch_samples\n",
        "                epoch_f1 = metrics['F1_Score'] / epoch_samples\n",
        "                    \n",
        "                # deep copy the model\n",
        "                if phase == 'val':\n",
        "                  if epoch_loss<best_loss:\n",
        "                    best_loss=epoch_loss\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                  if epoch_acc>best_acc:\n",
        "                    best_acc=epoch_acc\n",
        "                  if epoch_f1>best_f1:\n",
        "                    best_f1=epoch_f1               \n",
        "                       \n",
        "\n",
        "            time_elapsed = time.time() - since\n",
        "            print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "            #print('val loss: ',epoch_loss,' val acc: ',epoch_acc,' val f1 ',epoch_f1)\n",
        "\n",
        "        print('Cycle: {}'.format(i+1))\n",
        "        \n",
        "        print('Best val loss: {:4f}'.format(best_loss))\n",
        "        print('Best val acc: {:4f}'.format(best_acc))\n",
        "        print('Best val f1: {:4f}'.format(best_f1)) \n",
        "        print('*'*50)       \n",
        "        # load best model weights\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        snapshots.append(model.state_dict())\n",
        "    return snapshots\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHzY-1jXFiKF",
        "colab_type": "text"
      },
      "source": [
        "## Training SE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlO2S_awFuHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snapshots=None\n",
        "if SE==True:\n",
        "  snapshots=train_se(epochs_per_cycle, cycles, LR, dataloader, vis=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSzWbSJoHuID",
        "colab_type": "text"
      },
      "source": [
        "# Functionality for testing Snapshot ensembling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXKJ4wYQH1BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_se_pred(inputs,snapshots):\n",
        "    torch.cuda.empty_cache()\n",
        "    outputs=[]\n",
        "    for data in dataloader['test']:\n",
        "      for m in snapshots:\n",
        "        model.load_state_dict(m)\n",
        "        model.eval()\n",
        "        outputs.append(model(inputs))\n",
        "        torch.cuda.empty_cache()\n",
        "        print('done')\n",
        "    yhats = array(outputs)\n",
        "    res=np.sum(yhats,axis=0)\n",
        "\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Gg8F5Kuclj",
        "colab_type": "text"
      },
      "source": [
        "# Testing on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsiv1pJ2xRnX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_results(snapshots=None):\n",
        "  y_test=np.empty(0)\n",
        "  if snapshots is None:\n",
        "    model.eval()\n",
        "  for inputs in dataloader['test']:\n",
        "    inputs = inputs.to(device)\n",
        "    if snapshots is not None:\n",
        "      outputs=get_se_pred(inputs,snapshots)\n",
        "    else:\n",
        "      outputs = model(inputs)\n",
        "    softmax=torch.nn.Softmax();\n",
        "    outputs=softmax(outputs)\n",
        "    outputs=outputs.data.cpu().numpy()\n",
        "    outputs=np.argmax(outputs,axis=1)\n",
        "\n",
        "    outputs = le.inverse_transform(outputs.astype(int).ravel())\n",
        "    y_test=np.append(y_test,outputs)\n",
        "\n",
        "\n",
        "  return y_test\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEPgxCT_Hdx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test=get_test_results(snapshots)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLJHHgHNvUob",
        "colab_type": "text"
      },
      "source": [
        "# Saving and loading the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0hnf9PFvZbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if SE==False:\n",
        "  torch.save(model.state_dict(), saved_model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GoLZnIFxdBJ",
        "colab_type": "text"
      },
      "source": [
        "# .CSV submissions ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLuS_JWRx_Ri",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This is for top scoring csv file ensembles by Hard voting method.\n",
        "\n",
        "\"\"\"\n",
        "csvs=os.listdir('/content/drive/My Drive/ml/HackerEarthDanceMove/submissions')\n",
        "classes=['manipuri','bharatanatyam','odissi','kathakali','kathak','sattriya','kuchipudi','mohiniyattam']\n",
        "if len(csvs)>1:   \n",
        "    y_test=np.zeros(shape=(test_df.shape[0],y_train.shape[1]))\n",
        "\n",
        "\n",
        "    for sub in csvs:\n",
        "      cur_sub=pd.read_csv('/content/drive/My Drive/ml/HackerEarthDanceMove/submissions/'+sub)\n",
        "      cur_sub=cur_sub['target']\n",
        "      cur_sub=le.fit_transform(cur_sub)\n",
        "      cur_sub=ohe.fit_transform(cur_sub.reshape(-1,1))\n",
        "      # print(cur_sub.shape)\n",
        "      # print(cur_sub)\n",
        "      # break\n",
        "      for i,x in enumerate(cur_sub):\n",
        "        y_test[i]=y_test[i]+x\n",
        "        # print(y_test[i])\n",
        "      #   break\n",
        "      # break\n",
        "\n",
        "    #print(freq)\n",
        "    y_test=np.argmax(y_test,axis=1)\n",
        "    y_test = le.inverse_transform(y_test.astype(int).ravel())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFczxUIxugaJ",
        "colab_type": "text"
      },
      "source": [
        "# Making submission files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z-CLQOCKVLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.DataFrame( columns = ['Image','target'])\n",
        "submission['Image']=test_df['Image']\n",
        "submission[\"target\"]=y_test\n",
        "print(submission.head())\n",
        "submission.to_csv(saved_submission_file, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}